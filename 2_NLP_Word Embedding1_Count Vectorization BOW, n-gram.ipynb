{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP : Word Embedding_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer:\n",
    "It uses to two of the following models as the base to vectorize the given words on the basis of frequency of words.\n",
    "* it contains 2 models as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. BOW: Bag of Words Model:\n",
    "BOW model is used in NLP to represent the given text/sentence/documents as a collection (bag) of **words without giving any importance to grammar or the occurrence order of the words**. It keeps the account of frequency of the words in the text document, which can be used as features in many models.\n",
    "\n",
    "Let's understand this with an example:\n",
    "\n",
    "Text1 = \"I went to have a cup of coffee but I ended up having lunch with her.\"\n",
    "\n",
    "Text2 = \"I don't understand, what is the problem here?\"\n",
    "\n",
    "BOW1 = {I:2, went:1, to:1, have:1, a:1, cup:1, of:1, coffee:1, but:1, ended:1, up:1, having:1, lunch:1, with:1, her:1}\n",
    "\n",
    "BOW2= {I:1, don't:1, understand:1, what:1, is:1, the:1, problem:1, here:1}\n",
    "\n",
    "**- BOW is mainly used for feature selection:**\n",
    "* The above dictionary is converted as a list with *only the frequency terms there and on that basis*, weights are given to the most occurring terms. \n",
    "* But the \"stop words\" are the most frequent words that appears in raw document. \n",
    "* Thus, having a word with high frequency count doesn't mean that the word is as important. **To resolve this problem, \"Tf-idf\" was introduced.** We will discuss about it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. n-gram Model:\n",
    "\n",
    "As discussed in bag of words model, BOW model doesn't keep the sequence of words in a given text, only the frequency of words matters. It doesn't take into account the context of the given sentence, or care for grammatical rules such as verb is following a proper noun in the given text. n-gram model is used in such cases to keep the context of the given text intact.\n",
    "N-gram is the sequence of n words from a given text/document.\n",
    "\n",
    "When,\n",
    "\n",
    "    n=1, we call it a \"unigram\".\n",
    "    n=2, it is called a \"bigram\".\n",
    "    n=3, it is called a \"trigram\".\n",
    "    And so on...\n",
    "    \n",
    "Let's understand this with an example:\n",
    "\n",
    "Text1= \"I went to have a cup of coffee but I ended up having lunch with her.\"\n",
    ".\n",
    "\n",
    "* Unigram\n",
    "        [I, went, to, have, a, cup, of, coffee, but, I, ended, up, having, lunch, with, her]\n",
    "\n",
    "* Bi-gram\n",
    "        [I went],[went to],[to have],[have a],[a cup],[cup of],[of coffee],[coffee but],[butI],[Iended],[ended up],[up having],[having lunch],[lunch with],[with her]\n",
    "\n",
    "* Tri-gram\n",
    "        [I went to],[went to have],[to have a],[have a cup],[a cup of],[cup of coffee],[of coffee but],[coffee but I],[but I ended],[I ended up],[ended up having],[up having lunch],[having lunch with],[lunch with her].\n",
    "\n",
    "**Note:We can clearly see that BOW model is nothing but n-gram model when n=1.**\n",
    "\n",
    "Skip-grams\n",
    "Skip grams are type of n-grams where the words are not necessarily in the same order as are in the given text i.e. some words can be skipped. \n",
    "\n",
    "Example:\n",
    "\n",
    "Text2= \"I don't understand, what is the problem here?\"\n",
    "\n",
    "1-skip 2-grams (we have to make 2-gram while skipping 1 word)\n",
    "\n",
    "[I understand, don't what, understand is, what the, is problem, the here].\n",
    "\n",
    ".\n",
    "\n",
    "Let's see the implementation of Count vectorizer in python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW : Bag of Words\n",
    "* It is used to get the feature names **in Ascending order**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
     ]
    }
   ],
   "source": [
    "#Example of single document\n",
    "#Without stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "#Single document(',seperates each document)\n",
    "string=[\"This is an example of bag of words!\"]\n",
    "\n",
    "#This step will convert text into tokens\n",
    "vect1=CountVectorizer()\n",
    "\n",
    "vect1.fit_transform(string)\n",
    "print(\"bag of words\",vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 5, 'is': 3, 'an': 0, 'example': 2, 'of': 4, 'bag': 1, 'words': 6}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect1.vocabulary_ # .vocabulary_ method is used to get the indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and transform and predict if the word is present or not\n",
    "  â€¢This is widely used for document or subject classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vect= CountVectorizer()\n",
    "c_vect.fit(string)  #string=[\"This is an example of bag of words!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Present at [[0 0 0 2 1 0 1]]\n",
      "original indexes ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
     ]
    }
   ],
   "source": [
    "string2= ['Lets understand is of words is']\n",
    "\n",
    "c_new_vect= c_vect.transform(string2)\n",
    "\n",
    "print(\"Text Present at\",c_new_vect.toarray())\n",
    "\n",
    "#Compare with the indexes\n",
    "print(\"original indexes\",vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Explination**: here in string2 how many words are present which are already present in string (1)\n",
    "\n",
    "ex: \n",
    "- 'an' is not presennt in string2 so : 0\n",
    "- 'bag' is not presennt in string2 so : 0\n",
    "- 'example' is not presennt in string2 so : 0\n",
    "- 'is' is present and occured 2 times in string2 : 2\n",
    "\n",
    "and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
      "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
      "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
      "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
      "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
      "                            'itself', ...])\n"
     ]
    }
   ],
   "source": [
    "## Bag Of Words using stopwords(you can avoid writing extra steps to remove stopwords)\n",
    "# Count vectorizer itself has a stopwords: so i don't need to take care of stopwords specifically\n",
    "\n",
    "stpwords = stopwords.words('english')\n",
    "\n",
    "string = [\"This is an example of bag of words!\"]\n",
    "vect1=CountVectorizer(stop_words=stpwords)\n",
    "print(vect1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words: ['bag', 'example', 'words']\n",
      "vocab       : {'example': 1, 'bag': 0, 'words': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhimanyu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vect1.fit_transform(string)\n",
    "print(\"bag of words:\",vect1.get_feature_names())\n",
    "print(\"vocab       :\",vect1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Using function\n",
    "def text_matrix(message,countvect):\n",
    "    terms_doc = countvect.fit_transform(message)\n",
    "    return pd.DataFrame(terms_doc.toarray(),columns=countvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below metrix is the Bag of Words approach\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>but</th>\n",
       "      <th>for</th>\n",
       "      <th>get</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>language</th>\n",
       "      <th>making</th>\n",
       "      <th>mantra</th>\n",
       "      <th>natural</th>\n",
       "      <th>only</th>\n",
       "      <th>practice</th>\n",
       "      <th>processing</th>\n",
       "      <th>progress</th>\n",
       "      <th>slowly</th>\n",
       "      <th>success</th>\n",
       "      <th>the</th>\n",
       "      <th>there</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  but  for  get  in  is  language  making  mantra  natural  only  \\\n",
       "0    1    0    0    0   1   0         1       1       0        1     0   \n",
       "1    0    0    0    1   0   0         0       0       0        0     0   \n",
       "2    0    1    1    0   0   1         0       0       1        0     1   \n",
       "\n",
       "   practice  processing  progress  slowly  success  the  there  we  will  \n",
       "0         0           1         1       2        0    0      0   1     0  \n",
       "1         0           0         0       0        0    0      1   1     1  \n",
       "2         1           0         0       0        1    1      0   0     0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message=['We are slowly slowly making progress in Natural Language Processing',\n",
    "          \"We will get there\",\"But practice is the only mantra for success\"]\n",
    "\n",
    "c_vect = CountVectorizer()\n",
    "print(\"Below metrix is the Bag of Words approach\")\n",
    "text_matrix(message,c_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram   : ['an', 'example', 'gram', 'is', 'of', 'this']\n",
      "2-gram   : ['an example', 'example of', 'is an', 'of gram', 'this is']\n",
      "3-gram   : ['an example of', 'example of gram', 'is an example', 'this is an']\n",
      "4-gram   : ['an example of gram', 'is an example of', 'this is an example']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhimanyu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string=[\"This is an example of gram!\"]\n",
    "\n",
    "vect1 = CountVectorizer(ngram_range=(1,1)) # unnigram\n",
    "vect1.fit_transform(string)\n",
    "\n",
    "vect2 = CountVectorizer(ngram_range=(2,2)) # bigram\n",
    "vect2.fit_transform(string)\n",
    "# but what if we take (2,3): then it will take [bigram,trigram,bigram,trigram,...]\n",
    "\n",
    "vect3 = CountVectorizer(ngram_range=(3,3)) # trigram\n",
    "vect3.fit_transform(string)\n",
    "\n",
    "vect4 = CountVectorizer(ngram_range=(4,4))\n",
    "vect4.fit_transform(string)\n",
    "\n",
    "print(\"1-gram   :\",vect1.get_feature_names())\n",
    "print(\"2-gram   :\",vect2.get_feature_names())\n",
    "print(\"3-gram   :\",vect3.get_feature_names())\n",
    "print(\"4-gram   :\",vect4.get_feature_names())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
