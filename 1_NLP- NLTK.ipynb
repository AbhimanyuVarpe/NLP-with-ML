{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP : Natural Language Pracessing\n",
    "## nltk: Natural language Tool Kit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\abhimanyu\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\abhimanyu\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: regex in c:\\users\\abhimanyu\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: click in c:\\users\\abhimanyu\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\abhimanyu\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Tokenization:\n",
    "### 1. Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am Learning Natural language processing. Isn't it fun?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Learning', 'Natural', 'language', 'processing', '.', 'Is', \"n't\", 'it', 'fun', '?']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentence Tokenization (Segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Yeah. sure, I will do that. Even if it has 25.5% chance of success.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yeah.', 'sure, I will do that.', 'Even if it has 25.5% chance of success.']\n",
      "lenght of all the sentenses in text: 3\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))\n",
    "print(\"lenght of all the sentenses in text:\",len(sent_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions:  \n",
    "* It is a part of tokenization.\n",
    "* when we need any specific type of data rather than word_tokenized data. (ex. only small case data, only capital case data or only numerical data...etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLP is fun and Can deal with text and sounds, but can't deal with images. We have session at 11AM!. Done $.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'fun', 'and', 'an', 'deal', 'with', 'text', 'and', 'sounds', 'but', 'can', 't', 'deal', 'with', 'images', 'e', 'have', 'session', 'at', 'one']\n"
     ]
    }
   ],
   "source": [
    "# print word by word that contains all small case and starts from small a to z\n",
    "print(regexp_tokenize(text,\"[a-z]+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 's', 'f', 'u', 'n', 'a', 'n', 'd', 'a', 'n', 'd', 'e', 'a', 'l', 'w', 'i', 't', 'h', 't', 'e', 'x', 't', 'a', 'n', 'd', 's', 'o', 'u', 'n', 'd', 's', 'b', 'u', 't', 'c', 'a', 'n', 't', 'd', 'e', 'a', 'l', 'w', 'i', 't', 'h', 'i', 'm', 'a', 'g', 'e', 's', 'e', 'h', 'a', 'v', 'e', 's', 'e', 's', 's', 'i', 'o', 'n', 'a', 't', 'o', 'n', 'e']\n"
     ]
    }
   ],
   "source": [
    "# print word by word that contains all small case and starts from small a to z. without using \"+\" sign.\n",
    "print(regexp_tokenize(text,\"[a-z]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"+\" sign indicates **'whole word'**, without \"+\" the machine will take the same output but by seperating alphabet by alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'an',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'text',\n",
       " 'and',\n",
       " 'sounds',\n",
       " 'but',\n",
       " \"can't\",\n",
       " 'deal',\n",
       " 'with',\n",
       " 'images',\n",
       " 'e',\n",
       " 'have',\n",
       " 'session',\n",
       " 'at',\n",
       " 'one']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing same output but here with the extra quote \" ' \" example: can't, don't, I'll, etc.\n",
    "regexp_tokenize(text,\"[a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP', 'C', 'W', 'AM', 'D']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print word by word that contains all Capital cases and starts from cap A to Z.\n",
    "regexp_tokenize(text,\"[A-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"NLP is fun and Can deal with text and sounds, but can't deal with images. We have session at 11AM!. Done $.\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# everything in one line: \"\\\" with this symbol always use small a-z.\n",
    "regexp_tokenize(text,\"[\\a-z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' C',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ', ',\n",
       " ' ',\n",
       " \"'\",\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '. W',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' 11AM!. D',\n",
       " ' $.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anythig stars with caret \"^\" is 'not equals to':\n",
    "regexp_tokenize(text,\"[^a-z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only numbers:\n",
    "regexp_tokenize(text,\"[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"NLP is fun and Can deal with text and sounds, but can't deal with images. We have session at \",\n",
       " 'AM!. Done $.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without numbers:\n",
    "regexp_tokenize(text,\"[^0-9]+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Stop Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing stopwords:\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in another way:\n",
    "\n",
    "stopset = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'himself', 'your', 've', 'what', 'we', 'was', 'nor', 'their', 'been', 'own', \"mustn't\", 'he', \"it's\", 'over', 'there', 'is', 'be', 'because', 'wouldn', 'no', 'mightn', 'has', 'of', 'my', 'off', 'if', 'weren', 'those', 'against', 'under', 'doing', 'about', 'didn', 'for', 'very', 'as', 'with', 'on', 'his', 'it', 'ours', 'through', 'an', 'that', 'before', 'm', 'who', 'these', \"you'd\", 'him', 'mustn', 'were', 'where', 'here', 'itself', 'too', \"mightn't\", 'below', 'than', \"wasn't\", 'y', 'will', 'did', 'don', 'a', \"aren't\", 'down', 'why', 'not', 'doesn', 'now', 'while', 'same', 'such', 'by', 'i', 'into', 'above', 'some', 'shan', 'being', \"didn't\", \"doesn't\", \"should've\", 'further', \"shan't\", 'both', 'but', 'she', 'have', 'just', 'during', 'the', \"she's\", \"wouldn't\", \"you're\", 'll', \"isn't\", 'at', 'all', 'herself', 'can', 'in', 'wasn', 'couldn', \"won't\", 'other', 'only', 're', 'once', 'had', 'which', \"hadn't\", 'ourselves', 'each', 'shouldn', 'until', 'you', 'isn', 'its', 'more', 'hadn', 'yourselves', 'our', 'themselves', 'so', 'ain', 'does', 'most', 'am', 'them', 'whom', 'up', 'out', 'then', 'me', 'should', 'theirs', \"shouldn't\", 'to', 'do', 's', 'any', 'her', 'haven', 'when', 'having', 'this', 'again', \"needn't\", 'between', 'aren', 'how', 'yourself', 'myself', \"don't\", 'o', \"weren't\", 'few', 'from', 'or', 'won', 'yours', 'ma', 'and', \"that'll\", 'are', 't', 'hers', 'after', 'd', \"hasn't\", \"you've\", 'needn', 'they', \"couldn't\", 'hasn', \"you'll\", \"haven't\"}\n"
     ]
    }
   ],
   "source": [
    "print(stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **We can add or remove stopwords from default stopword set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding custom stop words:\n",
    "\n",
    "stopset.update(('new','old'))\n",
    "len(stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'himself', 'your', 've', 'old', 'what', 'we', 'was', 'nor', 'their', 'been', 'own', \"mustn't\", 'he', \"it's\", 'over', 'there', 'is', 'be', 'because', 'wouldn', 'no', 'mightn', 'has', 'of', 'my', 'off', 'if', 'weren', 'those', 'against', 'under', 'doing', 'about', 'didn', 'for', 'very', 'as', 'with', 'on', 'his', 'it', 'ours', 'through', 'an', 'that', 'before', 'm', 'who', 'these', \"you'd\", 'him', 'mustn', 'were', 'where', 'here', 'itself', 'too', \"mightn't\", 'below', 'than', \"wasn't\", 'y', 'will', 'did', 'don', 'a', \"aren't\", 'down', 'why', 'not', 'doesn', 'now', 'while', 'same', 'such', 'by', 'i', 'into', 'above', 'some', 'shan', 'being', \"didn't\", \"doesn't\", \"should've\", 'further', \"shan't\", 'both', 'but', 'she', 'have', 'just', 'during', 'the', \"she's\", \"wouldn't\", \"you're\", 'll', \"isn't\", 'at', 'all', 'herself', 'can', 'in', 'wasn', 'couldn', \"won't\", 'other', 'only', 're', 'once', 'had', 'which', \"hadn't\", 'ourselves', 'each', 'shouldn', 'until', 'you', 'isn', 'its', 'more', 'hadn', 'yourselves', 'our', 'themselves', 'so', 'ain', 'does', 'most', 'am', 'them', 'whom', 'up', 'out', 'then', 'me', 'should', 'new', 'theirs', \"shouldn't\", 'to', 'do', 's', 'any', 'her', 'haven', 'when', 'having', 'this', 'again', \"needn't\", 'between', 'aren', 'how', 'yourself', 'myself', \"don't\", 'o', \"weren't\", 'few', 'from', 'or', 'won', 'yours', 'ma', 'and', \"that'll\", 'are', 't', 'hers', 'after', 'd', \"hasn't\", \"you've\", 'needn', 'they', \"couldn't\", 'hasn', \"you'll\", \"haven't\"}\n"
     ]
    }
   ],
   "source": [
    "print(stopset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar to the stopwords we can ignore the punctuations in our sentenses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords and punctuations from above set\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "punct = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 32\n"
     ]
    }
   ],
   "source": [
    "print(len(stop_words), len(punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original lenth ==> 626\n",
      " Lenth of Cleaned tex ==> 59\n",
      "\n",
      " ['India', 'officially', 'Republic', 'India', 'Hindi', 'Bharat', 'country', 'South', 'Asia', 'It', 'seventh-largest', 'country', 'area', 'second-most', 'populous', 'country', 'populous', 'democracy', 'world', 'Bounded', 'Indian', 'Ocean', 'south', 'Arabian', 'Sea', 'southwest', 'Bay', 'Bengal', 'southeast', 'shares', 'land', 'borders', 'Pakistan', 'west', 'f', 'China', 'Nepal', 'Bhutan', 'north', 'Bangladesh', 'Myanmar', 'east', 'In', 'Indian', 'Ocean', 'India', 'vicinity', 'Sri', 'Lanka', 'Maldives', 'Andaman', 'Nicobar', 'Islands', 'share', 'maritime', 'border', 'Thailand', 'Myanmar', 'Indonesia']\n"
     ]
    }
   ],
   "source": [
    "# our text : \n",
    "\n",
    "text = \"India, officially the Republic of India (Hindi: Bharat), is a country in South Asia. It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west;[f] China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar and Indonesia.\"\n",
    "\n",
    "# empty list to load clean data:\n",
    "cleaned_text = []\n",
    "\n",
    "for word in nltk.word_tokenize(text):\n",
    "    if word not in punct:\n",
    "        if word not in stop_words:\n",
    "            cleaned_text.append(word)\n",
    "            \n",
    "print(f\"\"\" Original lenth ==> {len(text)}\"\"\")\n",
    "print(f\"\"\" Lenth of Cleaned tex ==> {len(cleaned_text)}\"\"\")\n",
    "print('\\n', cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india, officially the republic of india (hindi: bharat), is a country in south asia. it is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. bounded by the indian ocean on the south, the arabian sea on the southwest, and the bay of bengal on the southeast, it shares land borders with pakistan to the west;[f] china, nepal, and bhutan to the north; and bangladesh and myanmar to the east. in the indian ocean, india is in the vicinity of sri lanka and the maldives; its andaman and nicobar islands share a maritime border with thailand, myanmar and indonesia.\n",
      "\n",
      " INDIA, OFFICIALLY THE REPUBLIC OF INDIA (HINDI: BHARAT), IS A COUNTRY IN SOUTH ASIA. IT IS THE SEVENTH-LARGEST COUNTRY BY AREA, THE SECOND-MOST POPULOUS COUNTRY, AND THE MOST POPULOUS DEMOCRACY IN THE WORLD. BOUNDED BY THE INDIAN OCEAN ON THE SOUTH, THE ARABIAN SEA ON THE SOUTHWEST, AND THE BAY OF BENGAL ON THE SOUTHEAST, IT SHARES LAND BORDERS WITH PAKISTAN TO THE WEST;[F] CHINA, NEPAL, AND BHUTAN TO THE NORTH; AND BANGLADESH AND MYANMAR TO THE EAST. IN THE INDIAN OCEAN, INDIA IS IN THE VICINITY OF SRI LANKA AND THE MALDIVES; ITS ANDAMAN AND NICOBAR ISLANDS SHARE A MARITIME BORDER WITH THAILAND, MYANMAR AND INDONESIA.\n"
     ]
    }
   ],
   "source": [
    "# Convert it into Lower Case:\n",
    "print(text.lower())\n",
    "\n",
    "# Convert it into upper Case:\n",
    "print('\\n',text.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Stemming:\n",
    "Stemming means mapping the group of words to the same stem by removing Prefixes or Suffixeswithout giving any value to the 'grammatical meaning' of the stem formed after the process.\n",
    "\n",
    "\n",
    "example:\n",
    "* skiping ===> skip\n",
    "* skiped ===> skip\n",
    "* skips ===> skip\n",
    "\n",
    "* computation ===> comput\n",
    "* computer ===> comput\n",
    "\n",
    "\n",
    "We can see that stemming tries to bring the word back to their base word but the base word may or may not have correct gramatical meaning.\n",
    "\n",
    "\n",
    "There are few stemmer available in NLTK pakage. We will talk about popular two:\n",
    "1. Porter stemmer\n",
    "2. Lancaster Stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer:\n",
      "                hobbi\n",
      "                hobbi\n",
      "                comput\n",
      "                comput\n",
      "\n",
      "******************************************* \n",
      "Lancaster Stemmer:\n",
      "                hobby\n",
      "                hobby\n",
      "                comput\n",
      "                comput\n",
      "\n",
      "******************************************* \n",
      "Snowball Stemmer:\n",
      "                hobbi\n",
      "                hobbi\n",
      "                comput\n",
      "                comput\n",
      "\n",
      "******************************************* \n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "print(f\"\"\"Porter Stemmer:\n",
    "                {porter.stem('Hobby')}\n",
    "                {porter.stem('Hobbies')}\n",
    "                {porter.stem('computer')}\n",
    "                {porter.stem('computation')}\n",
    "\n",
    "******************************************* \"\"\")\n",
    "\n",
    "print(f\"\"\"Lancaster Stemmer:\n",
    "                {lancaster.stem('Hobby')}\n",
    "                {lancaster.stem('Hobbies')}\n",
    "                {lancaster.stem('computer')}\n",
    "                {lancaster.stem('computation')}\n",
    "\n",
    "******************************************* \"\"\")\n",
    "\n",
    "print(f\"\"\"Snowball Stemmer:\n",
    "                {snowball.stem('Hobby')}\n",
    "                {snowball.stem('Hobbies')}\n",
    "                {snowball.stem('computer')}\n",
    "                {snowball.stem('computation')}\n",
    "\n",
    "******************************************* \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The English language contains an enormous and ever-growing number of words.\n",
      "tokenize sentence: ['The', 'English', 'language', 'contains', 'an', 'enormous', 'and', 'ever-growing', 'number', 'of', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lets see with a new sentence:\n",
    "sentence = \"The English language contains an enormous and ever-growing number of words.\"\n",
    "\n",
    "token = list(nltk.word_tokenize(sentence)) # Word tokenizing the sentense\n",
    "\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"tokenize sentence:\", token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the english languag contain an enorm and ever-grow number of word .\n",
      "the engl langu contain an enorm and ever-growing numb of word .\n",
      "the english languag contain an enorm and ever-grow number of word .\n"
     ]
    }
   ],
   "source": [
    "for stemmer in (porter, lancaster, snowball):\n",
    "    stemm = [stemmer.stem(t) for t in token]\n",
    "    print(\" \".join(stemm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lancaster:** \n",
    "* Lancaster algorithm is faster than porter stemmer but it is more complex.\n",
    "\n",
    "**Porter:**\n",
    "* Porter stemmer is the oldest algorithm present and was most popular to use.\n",
    "\n",
    "**Snowball:**\n",
    "* Snowball Stemmer also known as Porter2. It is the updated version on porter stemmer. and is currently the most popular stemming algorithem.\n",
    "* It is available for multiple languages as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Lemmatization:\n",
    "\n",
    "- It is no guarantee that after stemming, the word we get has a gramatical meaning.\n",
    "- so, Stemming has some limitations, thats why Lemmatization comes in to the picture.\n",
    "- Lemmatization also does the same thing as stemming and try to bring a word to its base form, but unlike shemming it keeps in account the actual meaning of the base word.(i.e. the base word belongs to any specific laguage)\n",
    "\n",
    "\n",
    "- Lemmatization is complex and takes more time than stemming.\n",
    "- **It should use only when the real meaning of words or the context is necessary for processing, else stemming should be preferred.**\n",
    "\n",
    "\n",
    "We use **WordNet Lemmatizer** for Lemmatization in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma lemmatizemer:\n",
      "                Hobby\n",
      "                Hobbies\n",
      "                computer\n",
      "                computation\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "print(f\"\"\"lemma lemmatizemer:\n",
    "                {lemma.lemmatize('Hobby')}\n",
    "                {lemma.lemmatize('Hobbies')}\n",
    "                {lemma.lemmatize('computer')}\n",
    "                {lemma.lemmatize('computation')}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma lemmatizemer:\n",
      "                Running\n",
      "                Runs\n",
      "                Ran\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"lemma lemmatizemer:\n",
    "                {lemma.lemmatize('Running')}\n",
    "                {lemma.lemmatize('Runs')}\n",
    "                {lemma.lemmatize('Ran')}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see the lemma hasn't changed for the words with same base. this is because we haven't given any context to the lemmatizer.\n",
    "\n",
    "Generally it is given by passing the POS (Part Of Speech) tags for the words in a sentence. example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma lemmatizemer:\n",
      "                run\n",
      "                run\n",
      "                run\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"lemma lemmatizemer:\n",
    "                {lemma.lemmatize('running', pos='v')}\n",
    "                {lemma.lemmatize('runs', pos='v')}\n",
    "                {lemma.lemmatize('ran', pos='v')}\"\"\") # but when we use capital 'R' it wont work as it's a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for Bring is ===> bring\n",
      "Stemming for king is ===> king\n",
      "Stemming for Going is ===> go\n",
      "Stemming for Anything is ===> anyth\n",
      "Stemming for Sing is ===> sing\n",
      "Stemming for Ring is ===> ring\n",
      "Stemming for Nothing is ===> noth\n",
      "Stemming for Thing is ===> thing\n"
     ]
    }
   ],
   "source": [
    "# lets see anothe example:\n",
    "\n",
    "text = \"Bring king Going Anything Sing Ring Nothing Thing\"\n",
    "\n",
    "\n",
    "# Stemming:\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "\n",
    "for w in tokenization:\n",
    "    print(f\"\"\"Stemming for {w} is ===> {porter.stem(w)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for Bring is ===> Bring\n",
      "* Lemma for POS for Bring is ===> Bring\n",
      "Lemma for king is ===> king\n",
      "* Lemma for POS for king is ===> king\n",
      "Lemma for Going is ===> Going\n",
      "* Lemma for POS for Going is ===> Going\n",
      "Lemma for Anything is ===> Anything\n",
      "* Lemma for POS for Anything is ===> Anything\n",
      "Lemma for Sing is ===> Sing\n",
      "* Lemma for POS for Sing is ===> Sing\n",
      "Lemma for Ring is ===> Ring\n",
      "* Lemma for POS for Ring is ===> Ring\n",
      "Lemma for Nothing is ===> Nothing\n",
      "* Lemma for POS for Nothing is ===> Nothing\n",
      "Lemma for Thing is ===> Thing\n",
      "* Lemma for POS for Thing is ===> Thing\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization:\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "for w in tokenization:\n",
    "    print(f\"\"\"Lemma for {w} is ===> {wordnet.lemmatize(w)}\n",
    "* Lemma for POS for {w} is ===> {wordnet.lemmatize(w, pos='v')}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Wordnet:\n",
    "* wordnet is a nltk corpus reader, a lexical database for English. It can be **used to find the meaning of words, synonyms or antonym**. One can define it as a semantically oriented dictionary of English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms ===> {'alive', 'combat-ready', 'participating', 'active_agent', 'dynamic', 'active_voice', 'active', 'fighting'}\n",
      "Antonyms ===> {'passive', 'extinct', 'stative', 'passive_voice', 'dormant', 'inactive', 'quiet'}\n"
     ]
    }
   ],
   "source": [
    "# Lets find synonms and antonyms using python:\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets('active'):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append (l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "print(f\"\"\"Synonyms ===> {set(synonyms)}\n",
    "Antonyms ===> {set(antonyms)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms ===> {'principal_sum', 'principal', 'corpus'}\n",
      "Antonyms ===> set()\n"
     ]
    }
   ],
   "source": [
    "# Lets find synonms and antonyms using python:\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets('Corpus'):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append (l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "print(f\"\"\"Synonyms ===> {set(synonyms)}\n",
    "Antonyms ===> {set(antonyms)}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
